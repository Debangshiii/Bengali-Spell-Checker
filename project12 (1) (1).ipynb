{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7726754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check1(w1):\n",
    "    punc = '''!()\"\",।'''\n",
    "    for i in w1:\n",
    "        if i in punc:\n",
    "            w1 = w1.replace(i,\" \")\n",
    "    p=\"’-\"\n",
    "    for i in w1:\n",
    "        if i in p:\n",
    "            w1 = w1.replace(i,\"\")  \n",
    "    from bnlp import NLTKTokenizer\n",
    "    bnltk=NLTKTokenizer()\n",
    "    text=w1\n",
    "    tokens=bnltk.word_tokenize(text)\n",
    "    bigram=[]\n",
    "    n=len(tokens)\n",
    "    for i in range(n-1):\n",
    "        bigram.append(tokens[i:i+2])\n",
    "    with open(\"bigramdict.txt\",\"r\",encoding=\"utf-8\") as fi:\n",
    "            text=fi.read().split(\"\\n\")\n",
    "    bi=[]\n",
    "    for j in bigram:\n",
    "        st=\" \".join(j)\n",
    "        st=st+\" \"\n",
    "        bi.append(st)\n",
    "    c=[]\n",
    "    m=len(text)\n",
    "    dct={}\n",
    "    for i in range(m):\n",
    "        for j in bi:\n",
    "            n=len(j)\n",
    "            o=n+3\n",
    "            if text[i][:n]==j:\n",
    "                dct[j]=text[i][o:]\n",
    "                c.append(j)\n",
    "    e=[]\n",
    "    for i in bi:\n",
    "        if i not in c:\n",
    "             e.append(i)\n",
    "    st1=\" \"\n",
    "    for i in e:\n",
    "        st1+=i\n",
    "    words=st1.split()\n",
    "    wor=[]\n",
    "    for i in range(0, len(words)):  \n",
    "        count = 1;  \n",
    "        for j in range(i+1, len(words)):  \n",
    "            if(words[i] == (words[j])):  \n",
    "                count = count + 1   \n",
    "                words[j] = \"0\";  \n",
    "        if(count > 1 and words[i] != \"0\"):  \n",
    "                s=words[i]\n",
    "                wor.append(s)\n",
    "    n1=len(wor)\n",
    "    if n1!=0:\n",
    "        prr={}\n",
    "        from transformers import pipeline\n",
    "        model = pipeline('fill-mask', model=\"bert-base-multilingual-uncased\")\n",
    "        for i in tokens:\n",
    "            if i in wor:\n",
    "                w1=w1.replace(i,\"[MASK]\")\n",
    "        pred=model(w1)\n",
    "     #   for i in range(5): \n",
    "      #      prr[pred[i][\"token_str\"]]=pred[i][\"score\"]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd59c41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check(text):\n",
    "    sen=text\n",
    "    p='''’'''\n",
    "    for i in sen:\n",
    "        if i in p:\n",
    "            sen=sen.replace(i,\"\")\n",
    "    sen\n",
    "    w1=sen\n",
    "    w0=sen\n",
    "    punc = '''!()\"\",।'''\n",
    "    for i in w0:\n",
    "        if i in punc:\n",
    "            w0 = w0.replace(i,\" \")\n",
    "    from bnlp import NLTKTokenizer\n",
    "    bnltk=NLTKTokenizer()\n",
    "    text=w0\n",
    "    wordt=bnltk.word_tokenize(text)\n",
    "    sentence_tokens=bnltk.sentence_tokenize(text)\n",
    "    wt=[]\n",
    "    for i in wordt:\n",
    "        x=i.isnumeric()\n",
    "        if x!=True:\n",
    "            wt.append(i)\n",
    "    fh=open(\"C:\\\\Users\\\\shaun\\\\Downloads\\BengaliWordList_439.txt\",\"r\",encoding=\"utf-8\")\n",
    "    L=[]\n",
    "    s=\" \"\n",
    "    l=[]\n",
    "    er=[]\n",
    "    while(s):\n",
    "        s=fh.readline()\n",
    "        L=s.split()\n",
    "        for i in wt:\n",
    "                    if i in L:\n",
    "                        l.append(i)\n",
    "    for i in wt:\n",
    "                if i not in l:\n",
    "                        er.append(i)\n",
    "    cr=[]\n",
    "    for i in wt:\n",
    "                if i not in er:\n",
    "                        cr.append(i)\n",
    "    from bangla_stemmer.stemmer import stemmer\n",
    "    wordlist =er\n",
    "    stmr = stemmer.BanglaStemmer()\n",
    "    stm = stmr.stem(wordlist)\n",
    "    h=open(\"C:\\\\Users\\\\shaun\\\\Downloads\\BengaliWordList_439.txt\",\"r\",encoding=\"utf-8\")\n",
    "    c=[]\n",
    "    e=[]\n",
    "    z=' '\n",
    "    while(z):\n",
    "        z=h.readline()\n",
    "        Z=z.split()\n",
    "        for i in stm:\n",
    "            if i in Z :\n",
    "                c.append(i)\n",
    "    for i in stm:\n",
    "        if i not in c:\n",
    "            if i not in e:\n",
    "                e.append(i)\n",
    "    n=len(e)\n",
    "    s1=[]\n",
    "    for i in c:\n",
    "        if i not in s1:\n",
    "             s1.append(i)\n",
    "    print(s1)\n",
    "    def ed(s1,s2):\n",
    "        m=len(s1)\n",
    "        n=len(s2)\n",
    "        if m>n:\n",
    "            diff=m-n\n",
    "            for i in range(n):\n",
    "                if s1[i] != s2[i]:\n",
    "                     diff += 1\n",
    "        elif n>m:\n",
    "            diff=n-m\n",
    "            for i in range(m):\n",
    "                if s1[i] != s2[i]:\n",
    "                    diff += 1\n",
    "        else:\n",
    "            diff=0\n",
    "        return diff\n",
    "    n=len(e)\n",
    "    if n!=0:\n",
    "        gh=open(\"C:\\\\Users\\\\shaun\\\\Downloads\\BengaliWordList_439.txt\",\"r\",encoding=\"utf-8\")\n",
    "        ze=' '\n",
    "        err=[]\n",
    "        crr={}\n",
    "        while(ze):\n",
    "            ze=gh.readline()\n",
    "            M=ze.split()\n",
    "            for i in e:\n",
    "                for j in M:\n",
    "                    if ed(i,j)==1:\n",
    "                         crr[i]=j\n",
    "    return crr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ef817cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "\n",
    "def check_spelling():\n",
    "    suggestions={}\n",
    "    text = text_input.get(\"1.0\", \"end-1c\")  # Retrieve text from input area\n",
    "    # Perform spell-checking logic here and get suggestions\n",
    "    suggestions = spell_check1(text)\n",
    "    # Display suggestions in a messagebox or separate listbox\n",
    "    messagebox.showinfo(\"Spell Check Suggestions\", \"{}\\n\".format(suggestions))\n",
    "\n",
    "# Create the main application window\n",
    "window = tk.Tk()\n",
    "window.title(\"Bengali Spell Checker\")\n",
    "\n",
    "# Create a text input area\n",
    "text_input = tk.Text(window)\n",
    "text_input.pack()\n",
    "\n",
    "# Create a spell-check button\n",
    "spell_check_button = tk.Button(window, text=\"Check Spelling\", command=check_spelling)\n",
    "spell_check_button.pack()\n",
    "\n",
    "# Run the main application loop\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07150cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
